#!/usr/bin/env python3
"""
Validate 404 Not Found entries from a broken links report.

This utility reads a text report generated by the broken links finder, re-checks
all links that were previously reported as returning "404 Not Found", and writes
an updated validation report with the refreshed status of each link.
"""

from __future__ import annotations

import argparse
import hashlib
import os
import re
import sys
import time
from datetime import datetime, timezone
from http import HTTPStatus
from typing import Any, Dict, Iterable, List, Optional, Tuple
from urllib.parse import urlparse

import requests


DEFAULT_USER_AGENT = "Mozilla/5.0 (compatible; BrokenLinksValidator/1.0)"


class PageSourceCollector:
    """Fetch and persist source pages where broken links were detected."""

    def __init__(
        self,
        session: requests.Session,
        output_dir: str,
        timeout: float,
        verbose: bool = False,
    ) -> None:
        self.session = session
        self.output_dir = output_dir
        self.timeout = timeout
        self.verbose = verbose
        self._cache: Dict[str, Tuple[Optional[str], Optional[str]]] = {}

    @staticmethod
    def _sanitize_component(component: str) -> str:
        """Convert arbitrary text into a filesystem-safe component."""
        sanitized = re.sub(r"[^A-Za-z0-9._-]+", "_", component).strip("_")
        return sanitized or "page"

    def _build_filename(self, url: str) -> str:
        """Create a unique filename for a captured source page."""
        parsed = urlparse(url)
        base = f"{parsed.netloc}{parsed.path}"
        hashed = hashlib.md5(url.encode("utf-8")).hexdigest()[:8]
        sanitized = self._sanitize_component(base)
        if len(sanitized) > 80:
            sanitized = sanitized[:80].rstrip("_")
        return f"{sanitized}_{hashed}.html"

    def __call__(
        self,
        url: str,
        content: Optional[str] = None,
        encoding: Optional[str] = None,
    ) -> Tuple[Optional[str], Optional[str]]:
        """
        Retrieve and store the HTML content for the provided URL.

        Returns a tuple of (path, error). When the capture succeeds, path is the
        absolute file path and error is None. When it fails, path is None and
        error contains a human-readable description.
        """
        if not url:
            return None, "Missing source URL."

        url = url.strip()
        if not url:
            return None, "Missing source URL."

        if url in self._cache:
            return self._cache[url]

        response_text: Optional[str] = content
        response_encoding: Optional[str] = encoding

        if response_text is None:
            if self.verbose:
                print(f"Fetching source page: {url}", flush=True)

            try:
                response = self.session.get(url, allow_redirects=True, timeout=self.timeout)
                response_text = response.text
                response_encoding = response.encoding
            except requests.RequestException as exc:
                result = (None, f"Failed to fetch source page: {exc}")
                self._cache[url] = result
                return result
        else:
            response_encoding = response_encoding or "utf-8"
            if self.verbose:
                print(f"Saving provided content for: {url}", flush=True)

        try:
            os.makedirs(self.output_dir, exist_ok=True)
        except OSError as exc:
            result = (None, f"Unable to create source directory '{self.output_dir}': {exc}")
            self._cache[url] = result
            return result

        filename = self._build_filename(url)
        path = os.path.join(self.output_dir, filename)

        encoding_to_use = response_encoding or "utf-8"
        text_to_write = response_text or ""

        try:
            with open(path, "w", encoding=encoding_to_use) as handle:
                handle.write(text_to_write)
        except OSError as exc:
            result = (None, f"Failed to write source file '{path}': {exc}")
            self._cache[url] = result
            return result

        absolute_path = os.path.abspath(path)
        result = (absolute_path, None)
        self._cache[url] = result
        return result
def parse_report(file_path: str) -> Tuple[List[str], List[Dict[str, Any]]]:
    """
    Parse a plain-text broken links report into header lines and entry metadata.

    Returns a tuple of (header_lines, entries). Header lines are emitted without
    trailing newline characters. Each entry dict contains the captured fields:
    - broken_link (str)
    - status (str)
    - found_on (str, optional)
    - depth (int or str, optional)
    - timestamp (str, optional)
    - extra (List[str], optional) for any additional lines within the block
    """
    header: List[str] = []
    entries: List[Dict[str, Any]] = []
    current: Optional[Dict[str, Any]] = None
    in_entries = False

    with open(file_path, "r", encoding="utf-8") as handle:
        for raw_line in handle:
            line = raw_line.rstrip("\n")

            if line.startswith("Broken Link: "):
                in_entries = True
                if current:
                    entries.append(current)
                current = {"broken_link": line[len("Broken Link: ") :].strip()}
                continue

            if current is None:
                if not in_entries:
                    header.append(line)
                continue

            if line.startswith("Status: "):
                current["status"] = line[len("Status: ") :].strip()
            elif line.startswith("Found On: "):
                current["found_on"] = line[len("Found On: ") :].strip()
            elif line.startswith("Depth: "):
                depth_value = line[len("Depth: ") :].strip()
                try:
                    current["depth"] = int(depth_value)
                except ValueError:
                    current["depth"] = depth_value
            elif line.startswith("Timestamp: "):
                current["timestamp"] = line[len("Timestamp: ") :].strip()
            elif line.startswith("---"):
                # separator between entries; nothing to record
                continue
            elif line.strip():
                current.setdefault("extra", []).append(line)

        if current:
            entries.append(current)

    return header, entries


def format_status_text(status_code: Optional[int], reason: Optional[str]) -> str:
    """Return a human-friendly status string for the validation results."""
    if status_code is None:
        return "Request Failed"

    phrase = (reason or "").strip()
    if not phrase:
        try:
            phrase = HTTPStatus(status_code).phrase
        except ValueError:
            phrase = ""

    if phrase:
        return f"{status_code} {phrase}"

    return str(status_code)


def perform_request(
    session: requests.Session, url: str, timeout: float
) -> Tuple[Optional[int], Optional[str], Optional[str]]:
    """
    Issue HTTP requests to determine the current status of a URL.

    Returns a tuple (status_code, reason, method_used).
    - status_code can be None when the request fails entirely.
    - reason contains the response reason phrase when available.
    - method_used indicates which HTTP method produced the result.
    """
    try:
        head_response = session.head(url, allow_redirects=True, timeout=timeout)
    except requests.RequestException:
        head_response = None

    if head_response is not None:
        if head_response.status_code in (405, 501):
            head_response = None  # Force GET fallback when HEAD not allowed
        elif head_response.status_code != 404 and head_response.status_code < 400:
            return head_response.status_code, head_response.reason, "HEAD"

    # Perform GET either to confirm the 404 or because HEAD failed
    try:
        get_response = session.get(url, allow_redirects=True, timeout=timeout)
        return get_response.status_code, get_response.reason, "GET"
    except requests.RequestException as exc:
        if head_response is not None:
            return head_response.status_code, head_response.reason, "HEAD"
        return None, str(exc), "GET"


def determine_outcome(status_code: Optional[int]) -> str:
    """
    Map an HTTP status (or failure) to a validation outcome string.

    Possible return values:
    - "still_broken": 404 confirmed
    - "resolved": status < 400 and not 404
    - "other_error": status >= 400 but not 404
    - "error": network/processing failure
    """
    if status_code is None:
        return "error"
    if status_code == 404:
        return "still_broken"
    if 200 <= status_code < 400:
        return "resolved"
    return "other_error"


def _build_link_candidates(url: str) -> List[str]:
    """Create possible substrings that may appear in HTML for the given link."""
    candidates: List[str] = []
    if not url:
        return candidates

    parsed = urlparse(url)
    path = parsed.path or ""
    if parsed.params:
        path = f"{path};{parsed.params}"

    full_url = url
    if full_url:
        candidates.append(full_url)

    if parsed.scheme and parsed.netloc:
        candidates.append(f"//{parsed.netloc}{path}")

    if path:
        candidates.append(path)

    if parsed.query:
        path_with_query = f"{path}?{parsed.query}"
        candidates.append(path_with_query)
        query_html = parsed.query.replace("&", "&amp;")
        candidates.append(f"{path}?{query_html}")
        if parsed.scheme and parsed.netloc:
            candidates.append(f"{parsed.scheme}://{parsed.netloc}{path}?{query_html}")
            candidates.append(f"//{parsed.netloc}{path}?{query_html}")

    return [candidate for candidate in dict.fromkeys(candidates) if candidate]


def _link_present_in_html(html: str, broken_link: str) -> Optional[bool]:
    """Determine whether the broken link (or its variants) is still referenced."""
    if not html:
        return None

    candidates = _build_link_candidates(broken_link)
    for candidate in candidates:
        if candidate and candidate in html:
            return True
    return False if candidates else None


def inspect_found_on_page(
    session: requests.Session,
    found_on_url: Optional[str],
    broken_link: str,
    timeout: float,
    collector: Optional[PageSourceCollector],
    cache: Dict[str, Dict[str, Any]],
    verbose: bool = False,
) -> Dict[str, Any]:
    """Fetch (or reuse) the page the link was found on and check if it still references the link."""
    result: Dict[str, Any] = {
        "source_path": None,
        "source_error": None,
        "reference_found": None,
        "fetch_error": None,
        "fetched_at": None,
    }

    if not found_on_url:
        return result

    found_on_url = found_on_url.strip()
    if not found_on_url:
        return result

    if found_on_url in cache:
        return cache[found_on_url]

    html_text = ""
    encoding = "utf-8"
    fetched_at = datetime.now(timezone.utc).isoformat()

    try:
        response = session.get(found_on_url, allow_redirects=True, timeout=timeout)
        html_text = response.text or ""
        encoding = response.encoding or encoding
        result["status_code"] = response.status_code
        if verbose:
            print(f"Scanning source page: {found_on_url} ({response.status_code})", flush=True)
    except requests.RequestException as exc:
        result["fetch_error"] = f"Failed to fetch source page: {exc}"
        if verbose:
            print(f"Failed to fetch source page {found_on_url}: {exc}", flush=True)

    if collector is not None:
        path, write_error = collector(found_on_url, html_text, encoding)
        if path:
            result["source_path"] = path
        if write_error:
            result["source_error"] = write_error

    reference_found = _link_present_in_html(html_text, broken_link)

    result["reference_found"] = reference_found
    result["fetched_at"] = fetched_at

    cache[found_on_url] = result
    return result


def validate_entries(
    entries: Iterable[Dict[str, Any]],
    session: requests.Session,
    timeout: float,
    delay: float,
    verbose: bool = False,
    source_collector: Optional[PageSourceCollector] = None,
) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:
    """
    Re-check all entries that were previously flagged as 404.

    Returns a tuple of (validated_entries, summary) where:
    - validated_entries mirrors the input entries but contains a new "validation"
      key with detailed results.
    - summary is a dict with aggregated counts for reporting (including duplicates
      skipped and links removed).

    When a source_collector is provided, each entry's "found_on" page is fetched
    (with caching) to confirm the broken link is still referenced and to capture
    the HTML source when requested.
    """
    entries_list = list(entries)
    unique_entries: List[Dict[str, Any]] = []
    seen_pairs: set[Tuple[Optional[str], Optional[str]]] = set()
    duplicates_skipped = 0

    for entry in entries_list:
        key = (entry.get("broken_link"), entry.get("found_on"))
        if key in seen_pairs:
            duplicates_skipped += 1
            continue
        seen_pairs.add(key)
        unique_entries.append(entry)

    entries_list = unique_entries
    validated: List[Dict[str, Any]] = []
    summary = {
        "total_entries": 0,
        "rechecked": 0,
        "still_broken": 0,
        "resolved": 0,
        "other_error": 0,
        "errors": 0,
        "duplicates_skipped": duplicates_skipped,
        "link_removed": 0,
    }
    found_on_cache: Dict[str, Dict[str, Any]] = {}

    status_prefix = "404"
    total_to_recheck = sum(
        1
        for entry in entries_list
        if isinstance(entry.get("status", ""), str)
        and entry["status"].strip().startswith(status_prefix)
    )
    rechecked_so_far = 0

    for entry in entries_list:
        summary["total_entries"] += 1
        status_text = entry.get("status", "")
        validation: Dict[str, Any] = {}

        if isinstance(status_text, str) and status_text.strip().startswith(status_prefix):
            summary["rechecked"] += 1
            rechecked_so_far += 1
            if verbose:
                total_display = total_to_recheck or summary["rechecked"]
                print(
                    f"[{rechecked_so_far}/{total_display}] Rechecking {entry['broken_link']}",
                    flush=True,
                )
            status_code, reason, method_used = perform_request(
                session, entry["broken_link"], timeout
            )
            outcome = determine_outcome(status_code)
            validation_timestamp = datetime.now(timezone.utc).isoformat()

            validation.update(
                {
                    "checked": True,
                    "status_code": status_code,
                    "status_text": format_status_text(status_code, reason),
                    "method": method_used,
                    "timestamp": validation_timestamp,
                    "outcome": outcome,
                }
            )

            if status_code is None:
                validation["error"] = reason

            source_details: Dict[str, Any] = {}
            if entry.get("found_on"):
                source_details = inspect_found_on_page(
                    session,
                    entry.get("found_on"),
                    entry["broken_link"],
                    timeout,
                    source_collector,
                    found_on_cache,
                    verbose=verbose,
                )
                if source_details.get("source_path"):
                    validation["source_path"] = source_details["source_path"]
                if source_details.get("source_error"):
                    validation["source_error"] = source_details["source_error"]
                if source_details.get("fetch_error"):
                    validation["source_fetch_error"] = source_details["fetch_error"]
                if source_details.get("reference_found") is not None:
                    validation["reference_found"] = source_details["reference_found"]
                if source_details.get("fetched_at"):
                    validation["reference_checked_at"] = source_details["fetched_at"]

                if source_details.get("reference_found") is False:
                    outcome = "link_removed"

            validation["outcome"] = outcome

            if outcome == "link_removed":
                summary["link_removed"] += 1
                validation.pop("error", None)
            elif outcome == "error":
                summary["errors"] += 1
            else:
                summary[outcome] += 1

            if delay > 0:
                time.sleep(delay)
        else:
            validation.update(
                {
                    "checked": False,
                    "status_code": None,
                    "status_text": status_text or "Unknown",
                    "method": None,
                    "timestamp": None,
                    "outcome": "skipped",
                }
            )

        enriched_entry = dict(entry)
        enriched_entry["validation"] = validation
        validated.append(enriched_entry)

    if verbose:
        print(
            "Validation complete: "
            f"{summary['resolved']} resolved, "
            f"{summary['still_broken']} still broken, "
            f"{summary['link_removed']} links removed, "
            f"{summary['other_error']} other HTTP errors, "
            f"{summary['errors']} validation errors, "
            f"{summary['duplicates_skipped']} duplicates skipped.",
            flush=True,
        )

    return validated, summary


def write_validated_report(
    output_path: str,
    header_lines: List[str],
    validated_entries: List[Dict[str, Any]],
    summary: Dict[str, Any],
) -> None:
    """Persist the validation results to a text file."""
    lines: List[str] = []
    output_dir = os.path.dirname(os.path.abspath(output_path))

    if header_lines:
        lines.extend(header_lines)

    if lines and lines[-1] != "":
        lines.append("")

    lines.extend(
        [
            "Validation Summary",
            f"Validated At: {summary['validated_at']}",
            f"Source Report: {summary['source_report']}",
            f"Total Entries: {summary['total_entries']}",
            f"Links Rechecked: {summary['rechecked']}",
            f"Duplicates Skipped: {summary.get('duplicates_skipped', 0)}",
            f"Links Removed: {summary.get('link_removed', 0)}",
            f"Still Broken: {summary['still_broken']}",
            f"Resolved: {summary['resolved']}",
            f"Other HTTP Errors: {summary['other_error']}",
            f"Validation Errors: {summary['errors']}",
            "--------------------------------------------------",
        ]
    )

    # Retain only entries that remain broken after validation
    still_broken_entries = [
        entry
        for entry in validated_entries
        if (entry.get("validation") or {}).get("outcome") == "still_broken"
    ]

    for entry in still_broken_entries:
        lines.append(f"Broken Link: {entry.get('broken_link', 'Unknown')}")
        lines.append(f"Original Status: {entry.get('status', 'Unknown')}")

        if entry.get("found_on"):
            lines.append(f"Found On: {entry['found_on']}")
        if entry.get("depth") is not None:
            lines.append(f"Depth: {entry['depth']}")
        if entry.get("timestamp"):
            lines.append(f"First Seen: {entry['timestamp']}")

        validation = entry.get("validation", {})
        outcome = validation.get("outcome", "unknown").replace("_", " ").title()
        lines.append(f"Validation Outcome: {outcome}")

        status_text = validation.get("status_text")
        if status_text:
            lines.append(f"Validation Status: {status_text}")

        method = validation.get("method")
        if method:
            lines.append(f"Validation Method: {method}")

        validation_timestamp = validation.get("timestamp")
        if validation_timestamp:
            lines.append(f"Validation Checked: {validation_timestamp}")

        source_path = validation.get("source_path")
        if source_path:
            try:
                relative_path = os.path.relpath(source_path, output_dir)
            except ValueError:
                relative_path = source_path
            lines.append(f"Source Saved To: {relative_path}")
        else:
            source_error = validation.get("source_error")
            if source_error:
                lines.append(f"Source Capture Error: {source_error}")

        error_text = validation.get("error")
        if error_text:
            lines.append(f"Validation Error: {error_text}")

        for extra_line in entry.get("extra", []) or []:
            lines.append(extra_line)

        lines.append("------------------------------")

    if not still_broken_entries:
        lines.append("No links remain broken after validation.")

    with open(output_path, "w", encoding="utf-8") as handle:
        handle.write("\n".join(lines).rstrip() + "\n")


def build_default_output_path(input_path: str) -> str:
    """Create a default output path alongside the input file."""
    directory, filename = os.path.split(os.path.abspath(input_path))
    base, ext = os.path.splitext(filename)
    suffix = "_validated"
    return os.path.join(directory, f"{base}{suffix}{ext or '.txt'}")


def parse_arguments(argv: Optional[List[str]] = None) -> argparse.Namespace:
    """Parse command-line arguments for the validation script."""
    parser = argparse.ArgumentParser(
        description="Re-check 404 links in a broken links report and generate a validated report."
    )
    parser.add_argument("input", help="Path to the broken links report to validate.")
    parser.add_argument(
        "-o",
        "--output",
        help="Where to write the validated report. Defaults to <input>_validated.txt.",
    )
    parser.add_argument(
        "--timeout",
        type=float,
        default=10.0,
        help="HTTP request timeout in seconds (default: 10).",
    )
    parser.add_argument(
        "--delay",
        type=float,
        default=0.0,
        help="Delay in seconds between validation requests (default: 0).",
    )
    parser.add_argument(
        "--source-dir",
        help=(
            "Directory where HTML snapshots of the pages containing broken links "
            "will be stored. Only pages for links that remain broken are captured."
        ),
    )
    parser.add_argument(
        "--user-agent",
        default=DEFAULT_USER_AGENT,
        help=f"Custom User-Agent header (default: {DEFAULT_USER_AGENT}).",
    )
    parser.add_argument(
        "-v",
        "--verbose",
        action="store_true",
        help="Enable verbose output to show validation progress.",
    )

    return parser.parse_args(argv)


def main(argv: Optional[List[str]] = None) -> int:
    """CLI entry point."""
    args = parse_arguments(argv)

    if not os.path.exists(args.input):
        print(f"Input report not found: {args.input}", file=sys.stderr)
        return 1

    header_lines, entries = parse_report(args.input)

    if not entries:
        print("No broken link entries found in the report.", file=sys.stderr)
        return 1

    session = requests.Session()
    session.headers.update({"User-Agent": args.user_agent})

    collector: Optional[PageSourceCollector] = None
    if args.source_dir:
        collector = PageSourceCollector(
            session, args.source_dir, timeout=args.timeout, verbose=args.verbose
        )

    validated_entries, summary = validate_entries(
        entries,
        session,
        args.timeout,
        args.delay,
        verbose=args.verbose,
        source_collector=collector,
    )

    summary["validated_at"] = datetime.now(timezone.utc).isoformat()
    summary["source_report"] = os.path.abspath(args.input)

    output_path = args.output or build_default_output_path(args.input)
    write_validated_report(output_path, header_lines, validated_entries, summary)

    print(f"Validated report written to {output_path}")
    return 0


if __name__ == "__main__":  # pragma: no cover - CLI invocation guard
    sys.exit(main())
